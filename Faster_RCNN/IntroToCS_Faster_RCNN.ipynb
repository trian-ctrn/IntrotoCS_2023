{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VNcFTBWMSjn6"
      },
      "source": [
        "## Stop Colab from disconnecting\n",
        "Paste the following code into the web console (Ctrl + Shift + I). This code will simulate a click on the “Connect” button every minute, which will keep the notebook active.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-JtOQj_JhibH",
        "outputId": "93523394-fb78-4ad0-cfb0-9a027a4cfc42"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "function ConnectButton(){\n",
        "  console.log(\"Connect pushed\");\n",
        "  document.querySelector(\"#top-toolbar > colab-connectbutton\").shadowRoot.querySelector(\"#connect\").click()\n",
        "}\n",
        "setInterval(ConnectButton,60000);\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iwrLzHScxxFM"
      },
      "source": [
        "##Set up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q0ViYIT0pfUl",
        "outputId": "95a0a58e-bdd1-496a-ad0a-386363257dbd"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ab9g2SJvQZaQ",
        "outputId": "482bbc64-6d6c-4e23-82bd-9b69bf5bfd49"
      },
      "outputs": [],
      "source": [
        "\n",
        "%cd /content/gdrive/MyDrive/IntrotoCS_2023/Faster_RCNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lIWba1bUmngm",
        "outputId": "6aed0abc-0e9f-48ec-a812-9734740fbca7"
      },
      "outputs": [],
      "source": [
        "# Download TorchVision repo to use some files from\n",
        "# references/detection\n",
        "!pip install pycocotools --quiet\n",
        "!pip install torchmetrics\n",
        "!pip install git+https://github.com/albumentations-team/albumentations.git\n",
        "!git clone https://github.com/pytorch/vision.git\n",
        "!git checkout v0.3.0\n",
        "\n",
        "!cp vision/references/detection/utils.py ./\n",
        "!cp vision/references/detection/transforms.py ./\n",
        "!cp vision/references/detection/coco_eval.py ./\n",
        "!cp vision/references/detection/engine.py ./\n",
        "!cp vision/references/detection/coco_utils.py ./"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DluMZlr3QNQv"
      },
      "outputs": [],
      "source": [
        "# Basic python and ML Libraries\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "# for ignoring warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# We will be reading images using OpenCV\n",
        "import cv2\n",
        "\n",
        "# xml library for parsing xml files\n",
        "from xml.etree import ElementTree as et\n",
        "\n",
        "# matplotlib for visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "\n",
        "# torchvision libraries\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision import transforms as torchtrans\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "\n",
        "# these are the helper libraries imported.\n",
        "from engine import train_one_epoch, evaluate\n",
        "import utils\n",
        "import transforms as T\n",
        "\n",
        "# image augmentations\n",
        "import albumentations as A\n",
        "from albumentations.pytorch.transforms import ToTensorV2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jKtBSurT8IdL"
      },
      "source": [
        "## Prepare Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "24ZZKf-gQTNV",
        "outputId": "93d6e60a-5b8d-4987-c8d5-a48a4fd13975"
      },
      "outputs": [],
      "source": [
        "# defining the files directory and testing directory\n",
        "train_dir = ['/content/gdrive/MyDrive/IntrotoCS_2023/ver12/train', '/content/gdrive/MyDrive/IntrotoCS_2023/ver12/val']\n",
        "test_dir = ['/content/gdrive/MyDrive/IntrotoCS_2023/ver12/val']\n",
        "class UAVImagesDataset(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, files_dir_list, width, height, transforms=None):\n",
        "        self.transforms = transforms\n",
        "        self.height = height\n",
        "        self.width = width\n",
        "\n",
        "        # sorting the images for consistency\n",
        "        # To get images, the extension of the filename is checked to be jpg\n",
        "        self.imgs = []\n",
        "        for files_dir in files_dir_list:\n",
        "          self.imgs.extend([os.path.join(files_dir, \"images\", image) for image in sorted(os.listdir(files_dir + \"/images\"))\n",
        "                        if image[-4:]=='.jpg' or image[-4:]=='.png'])\n",
        "\n",
        "        # classes: 0 index is reserved for background\n",
        "        self.classes = [_, 'tree']\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        image_path = self.imgs[idx]\n",
        "\n",
        "        # reading the images and converting them to correct size and color\n",
        "        img = cv2.imread(image_path, cv2.IMREAD_COLOR)\n",
        "        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
        "        img_res = cv2.resize(img_rgb, (self.width, self.height), cv2.INTER_AREA)\n",
        "        # diving by 255\n",
        "        img_res /= 255.0\n",
        "\n",
        "        # annotation file\n",
        "        file_path, annot_filename = os.path.split(image_path)\n",
        "        annot_filename = annot_filename[:-4] + \".xml\"\n",
        "        annot_file_path = os.path.join(file_path[:-6], \"labels\", annot_filename)\n",
        "\n",
        "        boxes = []\n",
        "        labels = []\n",
        "        tree = et.parse(annot_file_path)\n",
        "        root = tree.getroot()\n",
        "\n",
        "        # cv2 image gives size as height x width\n",
        "        wt = img.shape[1]\n",
        "        ht = img.shape[0]\n",
        "\n",
        "        # box coordinates for xml files are extracted and corrected for image size given\n",
        "        bbox = False\n",
        "        for member in root.findall('object'):\n",
        "            bbox = True\n",
        "            labels.append(self.classes.index(member.find('name').text))\n",
        "\n",
        "            # bounding box\n",
        "            xmin = int(member.find('bndbox').find('xmin').text)\n",
        "            xmax = int(member.find('bndbox').find('xmax').text)\n",
        "\n",
        "            ymin = int(member.find('bndbox').find('ymin').text)\n",
        "            ymax = int(member.find('bndbox').find('ymax').text)\n",
        "\n",
        "\n",
        "            xmin_corr = (xmin/wt)*self.width\n",
        "            xmax_corr = (xmax/wt)*self.width\n",
        "            ymin_corr = (ymin/ht)*self.height\n",
        "            ymax_corr = (ymax/ht)*self.height\n",
        "\n",
        "            boxes.append([xmin_corr, ymin_corr, xmax_corr, ymax_corr])\n",
        "\n",
        "        # convert boxes into a torch.Tensor\n",
        "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
        "\n",
        "        # getting the areas of the boxes\n",
        "        area = (boxes[:,3] - boxes[:,1]) * (boxes[:,2] - boxes[:,0])\n",
        "\n",
        "        # suppose all instances are not crowd\n",
        "        iscrowd = torch.zeros((boxes.shape[0],), dtype=torch.int64)\n",
        "\n",
        "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
        "\n",
        "\n",
        "        target = {}\n",
        "        target[\"boxes\"] = boxes\n",
        "        target[\"labels\"] = labels\n",
        "        target[\"area\"] = area\n",
        "        target[\"iscrowd\"] = iscrowd\n",
        "        # image_id\n",
        "        image_id = torch.tensor([idx])\n",
        "        target[\"image_id\"] = image_id\n",
        "\n",
        "\n",
        "        if self.transforms:\n",
        "\n",
        "            sample = self.transforms(image = img_res,\n",
        "                                     bboxes = target['boxes'],\n",
        "                                     labels = labels)\n",
        "\n",
        "            img_res = sample['image']\n",
        "            target['boxes'] = torch.Tensor(sample['bboxes'])\n",
        "\n",
        "\n",
        "\n",
        "        return img_res, target\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.imgs)\n",
        "\n",
        "\n",
        "# check dataset\n",
        "dataset = UAVImagesDataset(train_dir, 224, 224)\n",
        "print('length of dataset = ', len(dataset), '\\n')\n",
        "\n",
        "# getting the image and target for a test index.  Feel free to change the index.\n",
        "img, target = dataset[0]\n",
        "print(img.shape, '\\n',target)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7uBbOMkNQY5F"
      },
      "outputs": [],
      "source": [
        "# Function to visualize bounding boxes in the image\n",
        "\n",
        "def plot_img_bbox(img, target):\n",
        "    # plot the image and bboxes\n",
        "    # Bounding boxes are defined as follows: x-min y-min width height\n",
        "    fig, a = plt.subplots(1,1)\n",
        "    fig.set_size_inches(5,5)\n",
        "    a.imshow(img)\n",
        "    target['boxes'] = target['boxes'].cpu()\n",
        "    for box in (target['boxes']):\n",
        "        x, y, width, height  = box[0], box[1], box[2]-box[0], box[3]-box[1]\n",
        "        rect = patches.Rectangle((x, y),\n",
        "                                 width, height,\n",
        "                                 linewidth = 2,\n",
        "                                 edgecolor = 'r',\n",
        "                                 facecolor = 'none')\n",
        "\n",
        "        # Draw the bounding box on top of the image\n",
        "        a.add_patch(rect)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6DZQJnErQdle"
      },
      "outputs": [],
      "source": [
        "# Send train=True for training transforms and False for val/test transforms\n",
        "def get_transform(train):\n",
        "\n",
        "    if train:\n",
        "        return A.Compose([\n",
        "                            # A.augmentations.crops.transforms.BBoxSafeRandomCrop(erosion_rate=0.0, always_apply=False, p=1),\n",
        "                            # Modify this\n",
        "                            A.OneOf([\n",
        "                              A.HorizontalFlip(p=1),\n",
        "                              A.RandomRotate90(p=1),\n",
        "                              A.VerticalFlip(p=1)\n",
        "                            ], p=0.5),\n",
        "                            # A.Resize(1000, 1000),\n",
        "                            # ToTensorV2 converts image to pytorch tensor without div by 255\n",
        "                            ToTensorV2(p=1.0)\n",
        "                        ], bbox_params=A.BboxParams(format='pascal_voc', min_visibility=0.1, label_fields=['labels']))\n",
        "    else:\n",
        "        return A.Compose([\n",
        "                            ToTensorV2(p=1.0)\n",
        "                        ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJXEtOP-8jGL"
      },
      "source": [
        "## Model with backbone Resnet50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pl2UBtHqQbYt"
      },
      "outputs": [],
      "source": [
        "def get_object_detection_model(num_classes):\n",
        "\n",
        "    # load a model pre-trained pre-trained on COCO\n",
        "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
        "\n",
        "    # get number of input features for the classifier\n",
        "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "    # replace the pre-trained head with a new one\n",
        "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IPmCS8SNQoFO"
      },
      "outputs": [],
      "source": [
        "# use our dataset and defined transformations\n",
        "dataset = UAVImagesDataset(train_dir, 1024, 1024, transforms= get_transform(train=True))\n",
        "dataset_val = UAVImagesDataset(train_dir, 1024, 1024, transforms= get_transform(train=False))\n",
        "\n",
        "# split the dataset in train and test set\n",
        "torch.manual_seed(1)\n",
        "indices = torch.randperm(len(dataset)).tolist()\n",
        "\n",
        "# train test split\n",
        "test_split = 0.2\n",
        "tsize = int(len(dataset)*test_split)\n",
        "dataset = torch.utils.data.Subset(dataset, indices[:-tsize])\n",
        "dataset_val = torch.utils.data.Subset(dataset_val, indices[-tsize:])\n",
        "\n",
        "# define training and validation data loaders\n",
        "# Change num_workers\n",
        "data_loader = torch.utils.data.DataLoader(\n",
        "    dataset, batch_size=8, shuffle=True, num_workers=4,\n",
        "    collate_fn=utils.collate_fn)\n",
        "\n",
        "data_loader_val = torch.utils.data.DataLoader(\n",
        "    dataset_val, batch_size=8, shuffle=False, num_workers=4,\n",
        "    collate_fn=utils.collate_fn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1PCVvPhwQp8W",
        "outputId": "f4c97edf-5a8f-40aa-8f62-3be26be2f4bc"
      },
      "outputs": [],
      "source": [
        "# to train on gpu if selected.\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "num_classes = 2\n",
        "\n",
        "# get the model using our helper function\n",
        "model = get_object_detection_model(num_classes)\n",
        "\n",
        "# move model to the right device\n",
        "model.to(device)\n",
        "\n",
        "# construct an optimizer\n",
        "params = [p for p in model.parameters() if p.requires_grad]\n",
        "optimizer = torch.optim.AdamW(params, lr = 0.0005, weight_decay=0.0005)\n",
        "\n",
        "# and a learning rate scheduler which decreases the learning rate like\n",
        "# a cosine with step at 5\n",
        "lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max = 5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H7gj-QVXQsJk",
        "outputId": "4c236d98-e26d-43ec-b55a-8dd8d0b43252"
      },
      "outputs": [],
      "source": [
        "# training for 20 epochs\n",
        "num_epochs = 50\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"Training epoch: {epoch + 1}/{num_epochs}\")\n",
        "    # training for one epoch\n",
        "    train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=10)\n",
        "    # update the learning rate\n",
        "    lr_scheduler.step()\n",
        "    # evaluate on the test dataset\n",
        "    evaluate(model, data_loader_val, device=device)\n",
        "\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O544Q59HAFUB",
        "outputId": "85f326f6-bb60-47c4-a10e-c927b35946e9"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GX-08shwQx0c"
      },
      "outputs": [],
      "source": [
        "# the function takes the original   and the iou threshold.\n",
        "\n",
        "def apply_nms(orig_prediction, iou_thresh=0.3):\n",
        "\n",
        "    # torchvision returns the indices of the bboxes to keep\n",
        "    keep = torchvision.ops.nms(orig_prediction['boxes'], orig_prediction['scores'], iou_thresh)\n",
        "\n",
        "    final_prediction = orig_prediction\n",
        "    final_prediction['boxes'] = final_prediction['boxes'][keep]\n",
        "    final_prediction['scores'] = final_prediction['scores'][keep]\n",
        "    final_prediction['labels'] = final_prediction['labels'][keep]\n",
        "\n",
        "    return final_prediction\n",
        "\n",
        "# function to convert a torchtensor back to PIL image\n",
        "def torch_to_pil(img):\n",
        "    return torchtrans.ToPILImage()(img).convert('RGB')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1hgr00_0Qzdz",
        "outputId": "14702c9c-d955-4c55-f6b2-69bd9891c25f"
      },
      "outputs": [],
      "source": [
        "# pick one image from the val set\n",
        "img, target = dataset_val[0]\n",
        "# put the model in evaluation mode\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    prediction = model([img.to(device)])[0]\n",
        "\n",
        "print('predicted #boxes: ', len(prediction['labels']))\n",
        "print('real #boxes: ', len(target['labels']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "iR4rC83sc9st",
        "outputId": "6d0df650-6328-40c2-e029-453df814fa3e"
      },
      "outputs": [],
      "source": [
        "from typing_extensions import final\n",
        "# Test set\n",
        "from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
        "from torchvision import ops\n",
        "\n",
        "map = MeanAveragePrecision(box_format=\"xyxy\", class_metrics=True)\n",
        "\n",
        "test_dataset = UAVImagesDataset(test_dir, 1024, 1024, transforms= get_transform(train=False))\n",
        "\n",
        "preds = []\n",
        "targets = []\n",
        "true_pos = 0\n",
        "false_pos = 0\n",
        "false_neg = 0\n",
        "total_label = 0\n",
        "for i in range(1, len(test_dataset)):\n",
        "  img, target = test_dataset[i]\n",
        "  targets.append(target)\n",
        "  # put the model in evaluation mode\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "      prediction = model([img.to(device)])[0]\n",
        "  nms_prediction = apply_nms(prediction, iou_thresh=0.01)\n",
        "\n",
        "  preds.append(nms_prediction)\n",
        "\n",
        "  target['boxes'] = target[\"boxes\"].to(device)\n",
        "  total_label += len(target['labels'])\n",
        "  num_pred = len(nms_prediction['labels'])\n",
        "  IoU = ops.box_iou(target[\"boxes\"], nms_prediction[\"boxes\"])\n",
        "  tp = 0\n",
        "  fn = 0\n",
        "  fp = 0\n",
        "  for result in IoU:\n",
        "    if torch.any(result > 0.4):\n",
        "      tp += 1\n",
        "    else:\n",
        "      fn += 1\n",
        "  fp = max(0, num_pred - tp)\n",
        "  true_pos += tp\n",
        "  false_pos += fp\n",
        "  false_neg += fn\n",
        "  print('EXPECTED OUTPUT\\n')\n",
        "  plot_img_bbox(torch_to_pil(img), target)\n",
        "  print('MODEL OUTPUT\\n')\n",
        "  plot_img_bbox(torch_to_pil(img), nms_prediction)\n",
        "\n",
        "print(\"Dataset length:\", len(test_dataset))\n",
        "print(\"Labels:\", total_label)\n",
        "precision = true_pos/(true_pos + false_pos)\n",
        "recall = true_pos/(true_pos + false_neg)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F_1 score:\", 2 * (precision * recall)/(precision + recall))\n",
        "\n",
        "map.update(preds=preds, target=targets)\n",
        "map.cpu()\n",
        "\n",
        "for k, v in map.compute().items():\n",
        "  print(f\"val_{k}: {v}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fc5-V74YvRiy",
        "outputId": "ffc8a62a-7644-4494-dbe4-358d19bbd919"
      },
      "outputs": [],
      "source": [
        "# Save model\n",
        "files_dir = r\"/content/gdrive/MyDrive/IntrotoCS_2023/Faster_RCNN/weights\"\n",
        "version = len([ver for ver in sorted(os.listdir(files_dir)) if ver[-3:] == \".pt\"]) # Check how many models have been saved\n",
        "torch.save(model.state_dict(), f\"{files_dir}/test({version}).pt\")\n",
        "print(f\"Saved to test({version}).pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RuZUfrhCv7PL",
        "outputId": "00302a11-ab87-4b9d-aa99-fcd64b2eb765"
      },
      "outputs": [],
      "source": [
        "# Load model\n",
        "num_classes = 2\n",
        "model = get_object_detection_model(num_classes)\n",
        "model.load_state_dict(torch.load(\"/content/gdrive/MyDrive/IntrotoCS_2023/Faster_RCNN/weights/test(15).pt\")) #Path to the model\n",
        "model.to(device)\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NunFWwPP_sh2"
      },
      "source": [
        "##Reference\n",
        "https://www.kaggle.com/code/yerramvarun/fine-tuning-faster-rcnn-using-pytorch\n",
        "https://medium.com/data-science-at-microsoft/how-to-smoothly-integrate-meanaverageprecision-into-your-training-loop-using-torchmetrics-7d6f2ce0a2b3"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "VNcFTBWMSjn6",
        "jKtBSurT8IdL"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
